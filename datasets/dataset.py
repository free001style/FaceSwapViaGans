import os.path as osp
from PIL import Image
from torch.utils.data import Dataset
import numpy as np
import torchvision.transforms as transforms
import random
import torchvision.transforms.functional as TF

# 19 attributes in total, skin-1,nose-2,...cloth-18, background-0
celelbAHQ_label_list = ['skin', 'nose', 'eye_g', 'l_eye', 'r_eye',
                        'l_brow', 'r_brow', 'l_ear', 'r_ear', 'mouth',
                        'u_lip', 'l_lip', 'hair', 'hat', 'ear_r',
                        'neck_l', 'neck', 'cloth']

# face-parsing.PyTorch also includes 19 attributesï¼Œbut with different permutation
face_parsing_PyTorch_label_list = ['skin', 'l_brow', 'r_brow', 'l_eye', 'r_eye',
                                   'eye_g', 'l_ear', 'r_ear', 'ear_r', 'nose',
                                   'mouth', 'u_lip', 'l_lip', 'neck', 'neck_l',
                                   'cloth', 'hair', 'hat']  # skin-1 l_brow-2 ...

# 9 attributes with left-right aggrigation
faceParser_label_list = ['background', 'mouth', 'eyebrows', 'eyes', 'hair',
                         'nose', 'skin', 'ears', 'belowface']

# 12 attributes with left-right aggrigation
faceParser_label_list_detailed = ['background', 'lip', 'eyebrows', 'eyes', 'hair',
                                  'nose', 'skin', 'ears', 'belowface', 'mouth',
                                  'eye_glass', 'ear_rings']

TO_TENSOR = transforms.ToTensor()
MASK_CONVERT_TF = transforms.Lambda(
    lambda celebAHQ_mask: __celebAHQ_masks_to_faceParser_mask(celebAHQ_mask))

MASK_CONVERT_TF_DETAILED = transforms.Lambda(
    lambda celebAHQ_mask: __celebAHQ_masks_to_faceParser_mask_detailed(celebAHQ_mask))

FFHQ_MASK_CONVERT_TF = transforms.Lambda(
    lambda mask: __ffhq_masks_to_faceParser_mask(mask))

FFHQ_MASK_CONVERT_TF_DETAILED = transforms.Lambda(
    lambda mask: __ffhq_masks_to_faceParser_mask_detailed(mask))

NORMALIZE = transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))


def get_transforms(normalize=True, toTensor=True):
    transform_list = []
    if toTensor:
        transform_list += [transforms.ToTensor()]

    if normalize:
        transform_list += [transforms.Normalize((0.5, 0.5, 0.5),
                                                (0.5, 0.5, 0.5))]
    return transforms.Compose(transform_list)


def __ffhq_masks_to_faceParser_mask_detailed(mask):
    """Convert the esitimated semantic image by face-parsing.PyTorch to reduced categories (12-class).

    Args:
        mask (PIL image): with shape [H,W]
    Return:
        aggrigated mask, with same shape [H,W] but the number of segmentation classes is less
    """

    converted_mask = np.zeros_like(mask)

    backgorund = np.equal(mask, 0)
    converted_mask[backgorund] = 0

    lip = np.logical_or(np.equal(mask, 12), np.equal(mask, 13))
    converted_mask[lip] = 1

    eyebrows = np.logical_or(np.equal(mask, 2),
                             np.equal(mask, 3))
    converted_mask[eyebrows] = 2

    eyes = np.logical_or(np.equal(mask, 4), np.equal(mask, 5))
    converted_mask[eyes] = 3

    hair = np.equal(mask, 17)
    converted_mask[hair] = 4

    nose = np.equal(mask, 10)
    converted_mask[nose] = 5

    skin = np.equal(mask, 1)
    converted_mask[skin] = 6

    ears = np.logical_or(np.equal(mask, 7), np.equal(mask, 8))
    converted_mask[ears] = 7

    belowface = np.equal(mask, 14)
    converted_mask[belowface] = 8

    mouth = np.equal(mask, 11)
    converted_mask[mouth] = 9

    eye_glass = np.equal(mask, 6)
    converted_mask[eye_glass] = 10

    ear_rings = np.equal(mask, 9)
    converted_mask[ear_rings] = 11

    return converted_mask


def __ffhq_masks_to_faceParser_mask(mask):
    """Convert the esitimated semantic image by face-parsing.PyTorch to reduced categories (9-class).

    Args:
        mask (PIL image): with shape [H,W]
    Return:
        aggrigated mask, with same shape [H,W] but the number of segmentation classes is less
    """
    converted_mask = np.zeros_like(mask)

    backgorund = np.equal(mask, 0)
    converted_mask[backgorund] = 0

    mouth = np.logical_or(
        np.logical_or(np.equal(mask, 11), np.equal(mask, 12)),
        np.equal(mask, 13)
    )
    converted_mask[mouth] = 1

    eyebrows = np.logical_or(np.equal(mask, 2),
                             np.equal(mask, 3))
    converted_mask[eyebrows] = 2

    eyes = np.logical_or(np.equal(mask, 4), np.equal(mask, 5))
    converted_mask[eyes] = 3

    hair = np.equal(mask, 17)
    converted_mask[hair] = 4

    nose = np.equal(mask, 10)
    converted_mask[nose] = 5

    skin = np.equal(mask, 1)
    converted_mask[skin] = 6

    ears = np.logical_or(np.equal(mask, 7), np.equal(mask, 8))
    converted_mask[ears] = 7

    belowface = np.equal(mask, 14)
    converted_mask[belowface] = 8

    return converted_mask


def __celebAHQ_masks_to_faceParser_mask_detailed(celebA_mask):
    """Convert the semantic image of CelebAMaskHQ to reduced categories (12-class).

    Args:
        mask (PIL image): with shape [H,W]
    Return:
        aggrigated mask, with same shape [H,W] but the number of segmentation classes is less
    """
    # 19 attributes in total, skin-1,nose-2,...cloth-18, background-0
    celelbAHQ_label_list = ['skin', 'nose', 'eye_g', 'l_eye', 'r_eye',
                            'l_brow', 'r_brow', 'l_ear', 'r_ear', 'mouth',
                            'u_lip', 'l_lip', 'hair', 'hat', 'ear_r',
                            'neck_l', 'neck', 'cloth']  # 12 attributes with left-right aggrigation
    faceParser_label_list_detailed = ['background', 'lip', 'eyebrows', 'eyes', 'hair',
                                      'nose', 'skin', 'ears', 'belowface', 'mouth',
                                      'eye_glass', 'ear_rings']

    converted_mask = np.zeros_like(celebA_mask)

    backgorund = np.equal(celebA_mask, 0)
    converted_mask[backgorund] = 0

    lip = np.logical_or(np.equal(celebA_mask, 11), np.equal(celebA_mask, 12))
    converted_mask[lip] = 1

    eyebrows = np.logical_or(np.equal(celebA_mask, 6),
                             np.equal(celebA_mask, 7))
    converted_mask[eyebrows] = 2

    eyes = np.logical_or(np.equal(celebA_mask, 4), np.equal(celebA_mask, 5))
    converted_mask[eyes] = 3

    hair = np.equal(celebA_mask, 13)
    converted_mask[hair] = 4

    nose = np.equal(celebA_mask, 2)
    converted_mask[nose] = 5

    skin = np.equal(celebA_mask, 1)
    converted_mask[skin] = 6

    ears = np.logical_or(np.equal(celebA_mask, 8), np.equal(celebA_mask, 9))
    converted_mask[ears] = 7

    belowface = np.equal(celebA_mask, 17)
    converted_mask[belowface] = 8

    mouth = np.equal(celebA_mask, 10)
    converted_mask[mouth] = 9

    eye_glass = np.equal(celebA_mask, 3)
    converted_mask[eye_glass] = 10

    ear_rings = np.equal(celebA_mask, 15)
    converted_mask[ear_rings] = 11

    return converted_mask


def __celebAHQ_masks_to_faceParser_mask(celebA_mask):
    """Convert the semantic image of CelebAMaskHQ to reduced categories (9-class).

    Args:
        mask (PIL image): with shape [H,W]
    Return:
        aggrigated mask, with same shape [H,W] but the number of segmentation classes is less
    """

    assert len(celebA_mask.size) == 2, "The provided mask should be with [H,W] format"

    converted_mask = np.zeros_like(celebA_mask)

    backgorund = np.equal(celebA_mask, 0)
    converted_mask[backgorund] = 0

    mouth = np.logical_or(
        np.logical_or(np.equal(celebA_mask, 10), np.equal(celebA_mask, 11)),
        np.equal(celebA_mask, 12)
    )
    converted_mask[mouth] = 1

    eyebrows = np.logical_or(np.equal(celebA_mask, 6),
                             np.equal(celebA_mask, 7))
    converted_mask[eyebrows] = 2

    eyes = np.logical_or(np.equal(celebA_mask, 4), np.equal(celebA_mask, 5))
    converted_mask[eyes] = 3

    hair = np.equal(celebA_mask, 13)
    converted_mask[hair] = 4

    nose = np.equal(celebA_mask, 2)
    converted_mask[nose] = 5

    skin = np.equal(celebA_mask, 1)
    converted_mask[skin] = 6

    ears = np.logical_or(np.equal(celebA_mask, 8), np.equal(celebA_mask, 9))
    converted_mask[ears] = 7

    belowface = np.equal(celebA_mask, 17)
    converted_mask[belowface] = 8

    return converted_mask


class CelebAHQDataset(Dataset):
    def __init__(self, dataset_root, mode="test",
                 img_transform=TO_TENSOR, label_transform=TO_TENSOR,
                 load_vis_img=False, fraction=1.0,
                 flip_p=-1):  # negative number for no flipping

        self.mode = mode
        self.root = dataset_root
        self.img_transform = img_transform
        self.label_transform = label_transform
        self.load_vis_img = load_vis_img
        self.fraction = fraction
        self.flip_p = flip_p

        if mode == "train":
            self.imgs = sorted([osp.join(self.root, "CelebA-HQ-img", "%d.jpg" % idx) for idx in range(28000)])
            self.labels = sorted([osp.join(self.root, "CelebA-HQ-mask", "%d.png" % idx) for idx in range(28000)])
            self.labels_vis = sorted(
                [osp.join(self.root, "vis", "%d.png" % idx) for idx in range(28000)]) if self.load_vis_img else None
        else:
            self.imgs = sorted([osp.join(self.root, "CelebA-HQ-img", "%d.jpg" % idx) for idx in range(28000, 30000)])
            self.labels = sorted([osp.join(self.root, "CelebA-HQ-mask", "%d.png" % idx) for idx in range(28000, 30000)])
            self.labels_vis = sorted([osp.join(self.root, "vis", "%d.png" % idx) for idx in
                                      range(28000, 30000)]) if self.load_vis_img else None

        self.imgs = self.imgs[:int(len(self.imgs) * self.fraction)]
        self.labels = self.labels[:int(len(self.labels) * self.fraction)]
        self.labels_vis = self.labels_vis[:int(len(self.labels_vis) * self.fraction)] if self.load_vis_img else None

        if self.load_vis_img:
            assert len(self.imgs) == len(self.labels) == len(self.labels_vis)
        else:
            assert len(self.imgs) == len(self.labels)

        # image pairs indices
        self.indices = np.arange(len(self.imgs))

    def __len__(self):
        return len(self.indices)

    def load_single_image(self, index):
        """Load one sample for training, inlcuding
            - the image,
            - the semantic image,
            - the corresponding visualization image

        Args:
            index (int): index of the sample
        Return:
            img: RGB image
            label: seg mask
            label_vis: visualization of the seg mask
        """
        img = self.imgs[index]
        img = Image.open(img).convert('RGB')
        if self.img_transform is not None:
            img = self.img_transform(img)

        label = self.labels[index]
        label = Image.open(label).convert('L')
        if self.label_transform is not None:
            label = self.label_transform(label)

        if self.load_vis_img:
            label_vis = self.labels_vis[index]
            label_vis = Image.open(label_vis).convert('RGB')
            label_vis = TO_TENSOR(label_vis)
        else:
            label_vis = -1  # unified interface
        return img, label, label_vis

    def __getitem__(self, idx):
        index = self.indices[idx]

        img, label, label_vis = self.load_single_image(index)

        if self.flip_p > 0:
            if random.random() < self.flip_p:
                img = TF.hflip(img)
                label = TF.hflip(label)

        return img, label, label_vis


class FFHQDataset(Dataset):
    def __init__(self, dataset_root,
                 img_transform=TO_TENSOR, label_transform=TO_TENSOR,
                 fraction=1.0,
                 load_raw_label=False,
                 flip_p=-1):

        self.root = dataset_root
        self.img_transform = img_transform
        self.label_transform = label_transform
        self.fraction = fraction
        self.load_raw_label = load_raw_label
        self.flip_p = flip_p

        with open(osp.join(self.root, "images_1024", "ffhq_list.txt"), "r") as f:
            f_lines = f.readlines()

        self.imgs = sorted([osp.join(self.root, "images_1024", line.replace("\n", "")) for line in f_lines])
        self.imgs = self.imgs[:int(len(self.imgs) * self.fraction)]
        self.labels = [img.replace("images_1024", "BiSeNet_mask") for img in self.imgs]

        assert len(self.imgs) == len(self.labels)

        self.indices = np.arange(len(self.imgs))

    def __len__(self):
        return len(self.indices)

    def load_single_image(self, index):
        """Load one sample for training, inlcuding
            - the image,
            - the semantic image,
            - the corresponding visualization image

        Args:
            index (int): index of the sample
        Return:
            img: RGB image
            label: seg mask
            label_vis: visualization of the seg mask
        """
        img = self.imgs[index]
        img = Image.open(img).convert('RGB')
        if self.img_transform is not None:
            img = self.img_transform(img)

        label = self.labels[index]
        label = Image.open(label).convert('L')

        if self.load_raw_label:
            original_label = TO_TENSOR(label)

        if self.label_transform is not None:
            label = self.label_transform(label)

        label_vis = -1  # unified interface

        if self.load_raw_label:
            return img, original_label, label, label_vis
        else:
            return img, label, label_vis

    def __getitem__(self, idx):
        index = self.indices[idx]

        img, label, label_vis = self.load_single_image(index)

        if self.flip_p > 0:
            if random.random() < self.flip_p:
                img = TF.hflip(img)
                label = TF.hflip(label)

        return img, label, label_vis
